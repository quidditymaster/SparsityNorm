{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your post here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"figure.figsize\":(12, 6),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class FixedSparsityRelu(Layer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sparsity,\n",
    "        alpha=0.95,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.sparsity = sparsity\n",
    "        self.alpha=alpha\n",
    "        super(FixedSparsityRelu, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\",\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=False,\n",
    "        )\n",
    "        super(FixedSparsityRelu, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        biased = tf.nn.bias_add(x, self.bias)\n",
    "        return tf.nn.relu(biased)\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_percentile_estimat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "n_classes = len(classes)\n",
    "\n",
    "x_train = x_train.astype(np.float32)/255.0\n",
    "x_test = x_test.astype(np.float32)/255.0\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = keras.layers.Input(x_train.shape[1:])\n",
    "x = x_in\n",
    "\n",
    "fsr_inputs = []\n",
    "fsr_layers = []\n",
    "\n",
    "x = L.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "cfsr = FixedSparsityRelu(sparsity=0.8)\n",
    "fsr_layers.append(cfsr)\n",
    "fsr_inputs.append(x)\n",
    "x = cfsr(x)\n",
    "\n",
    "x = L.MaxPooling2D(2)(x)\n",
    "\n",
    "x = L.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "cfsr = FixedSparsityRelu(sparsity=0.8)\n",
    "fsr_layers.append(cfsr)\n",
    "fsr_inputs.append(x)\n",
    "x = cfsr(x)\n",
    "\n",
    "x = L.MaxPooling2D(2)(x)\n",
    "\n",
    "x = L.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "cfsr = FixedSparsityRelu(sparsity=0.8)\n",
    "fsr_layers.append(cfsr)\n",
    "fsr_inputs.append(x)\n",
    "x = cfsr(x)\n",
    "\n",
    "x = L.GlobalMaxPooling2D()(x)\n",
    "x = L.Dense(n_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.models.Model(x_in, x)\n",
    "activations_model = keras.models.Model(x_in, fsr_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "fixed_sparsity_relu_6 (Fixed (None, 32, 32, 32)        32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "fixed_sparsity_relu_7 (Fixed (None, 16, 16, 64)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "fixed_sparsity_relu_8 (Fixed (None, 8, 8, 128)         128       \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 94,762\n",
      "Trainable params: 94,538\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    #rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "train_gen.fit(x_train)\n",
    "\n",
    "train_flow = train_gen.flow(x_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_dimensions(arr):\n",
    "    return arr.reshape((np.prod(arr.shape[:-1]), arr.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    #optimizer=keras.optimizers.Adam(0.001),\n",
    "    #optimizer=keras.optimizers.SGD(0.01, momentum=0.9, nesterov=True),\n",
    "    optimizer=keras.optimizers.Adadelta(0.5),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch 1\n",
      "metrics [2.203306  0.1846875]\n",
      "pre density [0.68804049 0.54264622 0.40980587]\n",
      "post density [0.64210515 0.50310621 0.38137913]\n",
      "avg bias value [-0.01716106 -0.01221351 -0.00673218]\n",
      "metrics [2.1183894  0.22601563]\n",
      "pre density [0.70214121 0.55237899 0.38942833]\n",
      "post density [0.62162426 0.48261304 0.34541788]\n",
      "avg bias value [-0.03923047 -0.03176828 -0.01817382]\n",
      "metrics [1.9799986  0.28757814]\n",
      "pre density [0.71690316 0.56005494 0.37476915]\n",
      "post density [0.60602412 0.46160838 0.31737582]\n",
      "avg bias value [-0.06308025 -0.05451123 -0.03044858]\n",
      "hold out evaluation\n",
      "10000/10000 [==============================] - 0s 49us/step\n",
      "[1.6755712629318238, 0.4163]\n",
      "\n",
      "\n",
      "epoch 2\n",
      "metrics [1.7894044 0.363125 ]\n",
      "pre density [0.74099907 0.57528152 0.36015089]\n",
      "post density [0.59086127 0.43625184 0.28836898]\n",
      "avg bias value [-0.11130512 -0.09986942 -0.05192274]\n",
      "metrics [1.7204853 0.3859375]\n",
      "pre density [0.75430503 0.58592813 0.35680544]\n",
      "post density [0.58706938 0.42645178 0.27860901]\n",
      "avg bias value [-0.14180703 -0.127518   -0.06431618]\n",
      "metrics [1.6757598 0.4008594]\n",
      "pre density [0.76754412 0.59719275 0.35661543]\n",
      "post density [0.58080683 0.41767833 0.2718404 ]\n",
      "avg bias value [-0.17463101 -0.15607132 -0.07711143]\n",
      "hold out evaluation\n",
      "10000/10000 [==============================] - 0s 44us/step\n",
      "[1.5895123046875, 0.4357]\n",
      "\n",
      "\n",
      "epoch 3\n",
      "metrics [1.6181277  0.42282814]\n",
      "pre density [0.78567134 0.61571092 0.3584811 ]\n",
      "post density [0.56923151 0.40573978 0.26333334]\n",
      "avg bias value [-0.23796217 -0.20774972 -0.10100862]\n",
      "metrics [1.6022809 0.427125 ]\n",
      "pre density [0.79382636 0.62507229 0.36111927]\n",
      "post density [0.56145399 0.39934592 0.25982346]\n",
      "avg bias value [-0.275385   -0.23644216 -0.11499086]\n",
      "metrics [1.580846   0.43703124]\n",
      "pre density [0.80071907 0.63420255 0.36377191]\n",
      "post density [0.55411075 0.39419492 0.25710953]\n",
      "avg bias value [-0.3144288  -0.26491714 -0.12939425]\n",
      "hold out evaluation\n",
      "10000/10000 [==============================] - 0s 45us/step\n",
      "[1.4559211978912354, 0.4785]\n",
      "\n",
      "\n",
      "epoch 4\n",
      "metrics [1.5677685  0.43948436]\n",
      "pre density [0.81045863 0.64494324 0.36631761]\n",
      "post density [0.54217322 0.3815222  0.2501946 ]\n",
      "avg bias value [-0.38865378 -0.3152166  -0.15507752]\n",
      "metrics [1.5569495  0.44647655]\n",
      "pre density [0.81552921 0.65144806 0.36774725]\n",
      "post density [0.53564314 0.37565044 0.24668064]\n",
      "avg bias value [-0.4325947  -0.3424656  -0.16860929]\n",
      "metrics [1.5522697  0.44902343]\n",
      "pre density [0.82011612 0.65716439 0.36951346]\n",
      "post density [0.52790604 0.36870371 0.24331288]\n",
      "avg bias value [-0.47754908 -0.3687005  -0.18133989]\n",
      "hold out evaluation\n",
      "10000/10000 [==============================] - 0s 44us/step\n",
      "[1.4437613864898682, 0.4866]\n",
      "\n",
      "\n",
      "epoch 5\n",
      "metrics [1.5534717  0.44519532]\n",
      "pre density [0.82765455 0.66523145 0.37169924]\n",
      "post density [0.51749594 0.35779529 0.23759382]\n",
      "avg bias value [-0.56150025 -0.41307124 -0.2024455 ]\n",
      "metrics [1.5428549 0.449375 ]\n",
      "pre density [0.83157138 0.66898363 0.37268652]\n",
      "post density [0.51171398 0.35212472 0.23481149]\n",
      "avg bias value [-0.6094881  -0.43614003 -0.21309726]\n",
      "metrics [1.544644   0.44792968]\n",
      "pre density [0.83534882 0.6724752  0.37398459]\n",
      "post density [0.50519236 0.34599327 0.23200097]\n",
      "avg bias value [-0.6582758  -0.45790696 -0.22305664]\n",
      "hold out evaluation\n",
      "10000/10000 [==============================] - 0s 45us/step\n",
      "[1.4878802074432373, 0.4651]\n",
      "\n",
      "\n",
      "epoch 6\n",
      "metrics [1.5436196  0.45034373]\n",
      "pre density [0.84147164 0.67784135 0.37576946]\n",
      "post density [0.49419988 0.33608741 0.22781429]\n",
      "avg bias value [-0.7477951  -0.49353638 -0.23919761]\n",
      "metrics [1.5388207  0.45467967]\n",
      "pre density [0.84449426 0.68040163 0.37669587]\n",
      "post density [0.48779888 0.33057437 0.22571947]\n",
      "avg bias value [-0.79824954 -0.51132756 -0.24729933]\n",
      "metrics [1.5395637  0.45390624]\n",
      "pre density [0.84727339 0.68251217 0.37728982]\n",
      "post density [0.48184269 0.32541944 0.22365779]\n",
      "avg bias value [-0.84902275 -0.52756125 -0.25478774]\n",
      "hold out evaluation\n",
      "10000/10000 [==============================] - 0s 39us/step\n",
      "[1.4710336112976075, 0.479]\n",
      "\n",
      "\n",
      "epoch 7\n",
      "metrics [1.5571202 0.4413125]\n",
      "pre density [0.85170275 0.68618275 0.37841389]\n",
      "post density [0.47033895 0.31632657 0.2200948 ]\n",
      "avg bias value [-0.9410011  -0.55297345 -0.26661256]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-937754657db7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mbias_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcavg_biases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mcloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mtraining_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2895\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_metrics = []\n",
    "pre_density_history = []\n",
    "post_density_history = []\n",
    "bias_history = []\n",
    "\n",
    "bias_calc_period = 20\n",
    "\n",
    "for epoch in range(51):\n",
    "    print(\"\\n\")\n",
    "    print(\"epoch\", epoch+1)\n",
    "    for batch_idx in range(50000//batch_size):\n",
    "        cx, cy = next(train_flow)\n",
    "        \n",
    "        if batch_idx % bias_calc_period == bias_calc_period-1:\n",
    "            activations = activations_model.predict(cx)\n",
    "            #import pdb; pdb.set_trace()\n",
    "            cur_predense = []\n",
    "            cur_postdense = []\n",
    "            cavg_biases = []\n",
    "            for act, fsr_layer in zip(activations, fsr_layers):\n",
    "                melted = melt_dimensions(act)\n",
    "                batch_optimal_biases = -1.0*np.percentile(melted, 100*fsr_layer.sparsity, axis=0)\n",
    "                current_bias = keras.backend.get_value(fsr_layer.bias)\n",
    "                new_bias = fsr_layer.alpha*current_bias + (1-fsr_layer.alpha)*batch_optimal_biases\n",
    "                cpre_density = np.mean(melted > 0)\n",
    "                cpost_density = np.mean(melted + new_bias > 0)\n",
    "                cur_predense.append(cpre_density)\n",
    "                cur_postdense.append(cpost_density)\n",
    "                #cavg_biases.append(np.mean(new_bias))\n",
    "                cavg_biases.append(np.mean(current_bias))\n",
    "                if True:\n",
    "                    keras.backend.set_value(fsr_layer.bias, new_bias)\n",
    "            pre_density_history.append(cur_predense)\n",
    "            post_density_history.append(cur_postdense)\n",
    "            bias_history.append(cavg_biases)\n",
    "        \n",
    "        closs = model.train_on_batch(cx, cy)\n",
    "        training_metrics.append(closs)\n",
    "        if batch_idx % 100 == 100-1:\n",
    "            print(\"metrics\", np.mean(training_metrics[-200:], axis=0))\n",
    "            print(\"pre density\", np.mean(pre_density_history[-200:], axis=0))\n",
    "            print(\"post density\", np.mean(post_density_history[-200:], axis=0))\n",
    "            print(\"avg bias value\", np.mean(bias_history[-200:], axis=0))\n",
    "    print(\"hold out evaluation\")\n",
    "    print(model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65536, 128)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02852320671081543"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(melted > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.40134739875793457,\n",
       " 0.02852320671081543,\n",
       " 0.11204409599304199,\n",
       " 0.05477714538574219,\n",
       " 0.022968292236328125,\n",
       " 0.09661865234375]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.mean(act > 0) for act in activations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nikola": {
   "author": "Tim Anderton",
   "category": "",
   "date": "2019-05-15 22:31:53 UTC-06:00",
   "description": "",
   "link": "",
   "slug": "replacing-neural-network-biases-with-a-sparsity-batch-norm",
   "tags": "",
   "title": "Replacing Neural Network Biases with a Sparsity Batch Norm",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
